Una de las primeras etapas de una canalización de datos es la transferencia de datos, que es donde se reciben grandes cantidades de datos en streaming. Sin embargo, los datos no siempre provienen de una única base de datos estructurada. En cambio, pueden fluir desde mil o incluso un millón de eventos diferentes que ocurren de forma asíncrona. Un ejemplo común de estos datos son las aplicaciones del IoT (Internet de las Cosas). Estas pueden incluir sensores en taxis que envían datos de ubicación cada 30 segundos, o sensores de temperatura en un centro de datos para optimizar la calefacción y la refrigeración.

PubSub es un servicio de mensajería distribuida que puede recibir mensajes de diversos flujos de dispositivos, como eventos de juegos, dispositivos IoT y flujos de aplicaciones. El nombre es la abreviatura de "publicador" y "suscriptor", o "publicar mensajes a suscriptores". Una vez capturados los mensajes de las fuentes de entrada en streaming, se necesita una forma de canalizar esos datos a un almacén de datos para su análisis. Aquí es donde entra en juego Dataflow.

Dataflow crea una canalización para procesar tanto datos en streaming como datos por lotes. El proceso, en este caso, se refiere a los pasos para extraer, transformar y cargar datos, a veces denominados ETL. Una solución popular para el diseño de pipelines es Apache Beam. Se trata de un modelo de programación unificado de código abierto que define y ejecuta pipelines de procesamiento de datos, incluyendo ETL, procesamiento por lotes y de flujo.

Dataflow gestiona gran parte de la complejidad de la configuración y el mantenimiento de la infraestructura, y se basa en la infraestructura de Google. Este producto permite un escalado automático fiable para satisfacer las demandas de los pipelines de datos. Dataflow no requiere servidor y está totalmente gestionado. La computación sin servidor permite a los desarrolladores de software crear y ejecutar aplicaciones sin tener que aprovisionar ni gestionar la infraestructura de backend. Por ejemplo, Google Cloud gestiona las tareas de infraestructura en nombre de los usuarios, como el aprovisionamiento de recursos, el ajuste del rendimiento y la garantía de la fiabilidad de los pipelines.

Un entorno totalmente gestionado es aquel en el que el software se puede implementar, supervisar y gestionar sin necesidad de un equipo de operaciones. Este entorno se puede crear mediante herramientas y tecnologías de automatización. Utilizar una solución sin servidor y totalmente gestionada como Dataflow permite dedicar más tiempo al análisis de la información de los conjuntos de datos y menos al aprovisionamiento de recursos para garantizar que los pipelines completen correctamente sus próximos ciclos.